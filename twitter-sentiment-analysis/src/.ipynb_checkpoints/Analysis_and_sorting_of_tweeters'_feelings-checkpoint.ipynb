{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkTt79exIvD5"
   },
   "source": [
    "#  Analyzing the sentiments of the people from micro-blogging site / social media sites (Twitter)\n",
    "\n",
    "\n",
    "### Dr. Abhay Bhadani\n",
    "#### Sr. Director/Head (Data Science)\n",
    "#### Yatra Online Ltd., Gurgaon\n",
    "#### Ph.D. (IIT Delhi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emotions are expressed in nuanced ways, which varies by collective or individual experiences, knowledge, and beliefs. Therefore, to understand emotion, as conveyed through text, a robust mechanism capable of capturing and modeling different linguistic nuances and phenomena is needed. Emotions reflect different users’ perspectives towards actions and events, therefore they are innately expressed in dynamic linguistic forms.\n",
    "\n",
    "\n",
    "Consider the social posts <B>“Thanks God for everything”</B> and <B>“Tnx mom for waaaaking me two hours early. Cant get asleep now”,</B> a lexicon-based model may not properly represent the emotion-relevant phrases: <I> “waaaaking me”, “Thanks God”, and “Tnx mom”. </I> First, the word “waaaaking” doesn’t exist in the English vocabulary, hence its referent may vary from its standard form, “waking”. Secondly, knowledge of the semantic similarity between the words <B>“Thanks” and “Tnx” </B> is needed to establish any relationship between the last two phrases. Even if such relationship can be established through knowledgebased techniques, it’s difficult to reliably determine the association of these phrases to a group of emotions. \n",
    "\n",
    "\n",
    "<B>Sentiment analysis is part of the Natural Language Processing (NLP).\n",
    "\n",
    "<B> It is a type of text mining which aims to determine the opinion and subjectivity of its content. \n",
    "    \n",
    "We can extract emotions related to some raw texts (e.g., reviews, comments, tweets). This is usually used on social media posts, customer reviews, customer queries, etc.  \n",
    "    \n",
    "Every customer facing industry (retail, telecom, finance, etc.) or political party or any such organizations are interested in identifying their customers’ sentiment, whether they think positive or negative, are they happy, sad, and so on about them.\n",
    "    \n",
    "Today, we shall perform a study to show how sentiment analysis can be performed using Python and how it can be deployed in production systems and host as an API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sen](./images/performing-twitter-sentiment-analysis1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOscR54nkfDe"
   },
   "source": [
    "###  Dataset:\n",
    "\n",
    "<!-- https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp -->\n",
    "\n",
    "<B>Description\n",
    "\n",
    "The data is in csv format. In computing, a comma-separated values (CSV) file stores tabular data (numbers and text) in plain text. Each line of the file is a data record. Each record consists of one or more fields, normally separated by commas. However, in this case the separator used in a semi-colon.\n",
    "\n",
    "\n",
    "Here, we will aggregate Tweets based on sentiment. The aggregation process is based on the association of tweets with the same feelings, as well as the degree and proportion of the feeling.\n",
    "\n",
    "The dataset consists of sentences that have been classified into the following categories: {'sadness', 'anger', 'love', 'surprise', 'fear', 'joy'}\n",
    "    \n",
    "    \n",
    "List of documents with emotion flag, Dataset is split into train, test & validation for building the machine learning model\n",
    "\n",
    "Example :-\n",
    "    \n",
    "    i feel like I am still looking at a blank canvas blank pieces of paper;sadness\n",
    "    \n",
    "    i cant walk into a shop anywhere where i do not feel uncomfortable;fear\n",
    "    \n",
    "    i felt anger when at the end of a telephone call;anger\n",
    "\n",
    "    i never make her separate from me because i don t ever want her to feel like i m ashamed with her;sadness\n",
    "    \n",
    "\n",
    "The methodology used is based on building a classifier using different algorithms (such as recurrent neural network) that is capable of analyzing sentiment, using a data set that includes a number of emotions.\n",
    "\n",
    "     \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "![pre](https://www.electronicsmedia.info/wp-content/uploads/2017/12/Data-Preprocessing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "<B> Approach\n",
    "    \n",
    "Text Cleaning Steps:\n",
    "    \n",
    "    1) Clean the data\n",
    "        Removing Twitter Handles (@user), Punctuations, Numbers, and Special Characters, Stop Words,\n",
    "        Removing Short Words\n",
    "    \n",
    "    2) Perform Tokenization:  \n",
    "            Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.\n",
    "    \n",
    "![vis](./images/tokenization.png)\n",
    "    \n",
    "    3) Stemming:\n",
    "            Stemming is a rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word. For example, For example – “play”, “player”, “played”, “plays” and “playing” are the different variations of the word – “play”\n",
    "    \n",
    "![vis](./images/stemming.jpeg)\n",
    "    \n",
    "    4) Visualization the Tweets using WordCloud:\n",
    "            A wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes.\n",
    "\n",
    "\n",
    "![vis](./images/word-cloud-sample.png)\n",
    "    \n",
    "\n",
    "The next stage involves using the trained model to sort tweets based on sentiment with a rating ratio.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this partial stage, we will follow two methodologies: \n",
    "    \n",
    "    The first is to draw a graph that shows the percentage of each of the feelings of the tweeters within Twitter regarding what is happening in the state of Sri Lanka.\n",
    "    \n",
    "    The next partial stage, is to move to the study of each of these feelings for the tweeters, and try to collect them in order to determine the degree of feelings for each of them.\n",
    "    \n",
    "    The final hierarchical schemas (for each one of the feelings) will show the correlation of the tweeters in terms of the degree of affiliation with that feeling.\n",
    "    \n",
    "The Euclidean distance will be used to calculate the degree of convergence for a single feeling (depending on the percentage of tweeting classification and belonging to a specific feeling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation of Words as Vectors\n",
    "\n",
    "There are various ways to represent words in Vector Format.\n",
    "    Bag-Of-Words\n",
    "    \n",
    "    Term Frequecy - Inverse Document Frequecy (TF-IDF)\n",
    "    \n",
    "    Word2Vec (Skip-Gram and CBOW)\n",
    "    \n",
    "    GloVe: GloVe stands for Global Vectors for word representation.\n",
    "  \n",
    "   \n",
    "    Fast-Text: FastText was introduced by Facebook back in 2016. The idea behind FastText is very similar to Word2Vec. However, there was still one thing that methods like Word2Vec and GloVe lacked. Even though both of these models have been trained on billions of words, that still means our vocabulary is limited. FastText improved over other methods because of its capability of generalization to unknown words, which had been missing all along in the other methods.\n",
    "    \n",
    "    Bidirectional Encoder Representations from Transformers (BERT): BERT is a transformer-based architecture. Transformer uses a self-attention mechanism, which is suitable for language understanding. BERT is a multi-layered encoder. \n",
    "        \n",
    "        BERT base – 12 layers, 12 attention heads, and 110 million parameters. \n",
    "        \n",
    "        BERT Large – 24 layers, 16 attention heads and, 340 million parameters.\n",
    "\n",
    "\n",
    "    \n",
    "    Sentence - Encoders\n",
    "\n",
    "\n",
    "<B>Vectorization</B> is jargon for a classic approach of converting input data from its raw format (i.e. text ) into vectors of real numbers which is the format that ML models support. This approach has been there ever since computers were first built, it has worked wonderfully across various domains, and it’s now used in NLP.\n",
    "\n",
    "\n",
    "Download a pretrained vector representation of the words. \n",
    "These pre-trained vectors have been trained using GloVe embedding technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![viz](./images/GloVe_Representation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdMrm2WGRhIz",
    "outputId": "40f2ca23-9f60-4ea5-976b-058165662f9b"
   },
   "outputs": [],
   "source": [
    "# !wget https://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOdFRHWtS8qj",
    "outputId": "f59ef44d-fb7d-46b4-f1de-961df8fcbe24"
   },
   "outputs": [],
   "source": [
    "# !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UouennyQlWL5"
   },
   "outputs": [],
   "source": [
    "# import opendatasets as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zTWC3B7q4hc"
   },
   "outputs": [],
   "source": [
    "# dataset_emotion = \"emotions-dataset-for-nlp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lJZaw7rmLVq"
   },
   "source": [
    "#### Install and import relevant python Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sklearn seaborn matplotlib tensorflow keras nltk flask requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXlUC8nS2Jhk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import nltk\n",
    "import string\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM ,Conv2D, Dense,GlobalAveragePooling1D,Flatten, Dropout , GRU, TimeDistributed, Conv1D, MaxPool1D, MaxPool2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ti78Jl77mOSQ"
   },
   "source": [
    "### First Step:\n",
    "building a recurrent neural network capable of analyzing emotions, using a dataset that includes a number of emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QTldoyaQ5-xc",
    "outputId": "0ce16bb5-0522-4d30-fc08-4b929b1df4ce"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "porter = PorterStemmer()\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hCaKP731THK"
   },
   "outputs": [],
   "source": [
    "class Emotion:\n",
    "  def __init__(self, datasetFolder, batch_size, validation_split, optimizer, loss, epochs):\n",
    "    self.datasetFolder = datasetFolder\n",
    "    self.batch_size = batch_size\n",
    "    self.validation_split = validation_split\n",
    "    self.optimizer = optimizer\n",
    "    self.loss = loss\n",
    "    self.epochs = epochs\n",
    "  def readDatasetCSV(self):\n",
    "    trainDataset = pd.read_csv(os.path.join(self.datasetFolder, \"data/train.txt\"), names=['Text', 'Emotion'], sep=';')\n",
    "    testDataset = pd.read_csv(os.path.join(self.datasetFolder, \"data/test.txt\"), names=['Text', 'Emotion'], sep=';')\n",
    "    validDataset = pd.read_csv(os.path.join(self.datasetFolder, \"data/val.txt\"), names=['Text', 'Emotion'], sep=';')\n",
    "    list_dataset = [trainDataset, testDataset, validDataset]\n",
    "    self.dataset = pd.concat(list_dataset)\n",
    "  def FeaturesLables(self):\n",
    "    self.features = self.dataset['Text']\n",
    "    self.labels = self.dataset['Emotion']  \n",
    "  def splitDataset(self):\n",
    "    self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.features,\n",
    "                                                                            self.labels, \n",
    "                                                                            test_size = self.validation_split)\n",
    "  def CleanFeatures(self):\n",
    "    self.features = self.features.apply(lambda sequence:\n",
    "                                              [ltrs.lower() for ltrs in sequence if ltrs not in string.punctuation])\n",
    "    self.features = self.features.apply(lambda wrd: ''.join(wrd))\n",
    "  def tokenizerDataset(self):\n",
    "    self.tokenizer = Tokenizer(num_words=5000)\n",
    "    self.tokenizer.fit_on_texts(self.features)\n",
    "    train = self.tokenizer.texts_to_sequences(self.features)\n",
    "    self.features = pad_sequences(train)\n",
    "    le = LabelEncoder()\n",
    "    self.labels = le.fit_transform(self.labels)\n",
    "    self.vocabulary = len(self.tokenizer.word_index)\n",
    "  def label_categorical(self):\n",
    "    self.labels = to_categorical(self.labels, 6)\n",
    "  def glove_word_embedding(self, file_name):\n",
    "    self.embeddings_index = {}\n",
    "    file_ = open(file_name)\n",
    "    for line in file_:\n",
    "        arr = line.split()\n",
    "        single_word = arr[0]\n",
    "        w = np.asarray(arr[1:],dtype='float32')\n",
    "        self.embeddings_index[single_word] = w\n",
    "    file_.close()\n",
    "    max_words = self.vocabulary + 1\n",
    "    word_index = self.tokenizer.word_index\n",
    "    self.embedding_matrix = np.zeros((max_words,300)).astype(object)\n",
    "    for word , i in word_index.items():\n",
    "            embedding_vector = self.embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                self.embedding_matrix[i] = embedding_vector  \n",
    "  def model(self):\n",
    "    m = Sequential()\n",
    "    m.add(Input(shape=(self.features.shape[1], )))\n",
    "    m.add(Embedding(self.vocabulary + 1,300))\n",
    "    m.add(GRU(128, recurrent_dropout=0.3, return_sequences=False, activity_regularizer = tf.keras.regularizers.L2(0.0001)))\n",
    "    m.add(Dense(6, activation=\"softmax\", activity_regularizer = tf.keras.regularizers.L2(0.0001)))\n",
    "    self.m = m\n",
    "  def compiler(self):\n",
    "    self.m.compile(loss= self.loss,optimizer=self.optimizer,metrics=['accuracy'])\n",
    "  def fit(self):\n",
    "    earlyStopping = EarlyStopping(monitor = 'loss', patience = 20, mode = 'min', restore_best_weights = True)\n",
    "    self.history_training = self.m.fit(self.X_train, self.Y_train, epochs= self.epochs,batch_size = self.batch_size,\n",
    "                                       callbacks=[ earlyStopping])   \n",
    "    \n",
    "  def save_model(self, model_file='./models/model.json'):\n",
    "    # serialize model to JSON\n",
    "    model_json = self.m.to_json()\n",
    "    with open(model_file, \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "        self.m.save_weights(model_file+\".h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lx7E5vh4JJj"
   },
   "outputs": [],
   "source": [
    "dataset_emotion = \".\"\n",
    "epochs = 1\n",
    "emotion = Emotion(dataset_emotion, 256, 0.1, 'adam', 'categorical_crossentropy', epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbeS33eK4ZGo"
   },
   "outputs": [],
   "source": [
    "emotion.readDatasetCSV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6LOwhoJd5G_y",
    "outputId": "f6e585c5-b1c4-4eae-c762-0801ddd422ef"
   },
   "outputs": [],
   "source": [
    "emotion.dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiXNZHgv68U5"
   },
   "outputs": [],
   "source": [
    "emotion.FeaturesLables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vwzhdol861C8"
   },
   "outputs": [],
   "source": [
    "emotion.CleanFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8W8kXcdj7D3V",
    "outputId": "a4a64f5b-bf51-4f67-c950-e1277ec4c420"
   },
   "outputs": [],
   "source": [
    "emotion.features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jboiwLYR-BH5",
    "outputId": "d5af60f6-2b8d-44b5-f1f7-14c996b54fbe"
   },
   "outputs": [],
   "source": [
    "emotion.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YjnxHLx27LzC"
   },
   "outputs": [],
   "source": [
    "emotion.tokenizerDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QadS6V1r7Nzy",
    "outputId": "3d925112-0091-4f8a-8fe5-82dbcf05c66b"
   },
   "outputs": [],
   "source": [
    "emotion.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fkl08uAy7Svb",
    "outputId": "dbf4de3f-076c-4d82-a5be-cd703ddd2a15"
   },
   "outputs": [],
   "source": [
    "emotion.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hNC3L4Ww7VGh",
    "outputId": "a98b7a8e-fb4c-4026-b277-9e732e426fd8"
   },
   "outputs": [],
   "source": [
    "emotion.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MTYzKXMq7pWE",
    "outputId": "689e81db-a764-4a00-f6fc-25a7d9611a94"
   },
   "outputs": [],
   "source": [
    "emotion.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6ZH_xy5-v8D"
   },
   "outputs": [],
   "source": [
    "emotion.label_categorical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mL6csMqQ_H56",
    "outputId": "d0aca25f-3e98-4afa-e472-df13618c47f6"
   },
   "outputs": [],
   "source": [
    "emotion.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGG0v5kgzhJ3"
   },
   "outputs": [],
   "source": [
    "emotion.splitDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYo7knqATXgE"
   },
   "outputs": [],
   "source": [
    "emotion.glove_word_embedding(\"./pre-trained-embeddings/glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ooT0Ia3Lror9",
    "outputId": "0a2ce248-e0ec-4a5e-b975-c61bd092c948"
   },
   "outputs": [],
   "source": [
    "emotion.model()\n",
    "emotion.m.layers[0].set_weights([emotion.embedding_matrix])\n",
    "emotion.m.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0GVniwDTfOi"
   },
   "outputs": [],
   "source": [
    "emotion.compiler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IktgYRNWBdd6",
    "outputId": "7d0e0696-1bb5-4cf5-8a51-19d0e2390529"
   },
   "outputs": [],
   "source": [
    "emotion.m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PHpg5OqJBpqQ",
    "outputId": "e1355567-ebb8-4fe2-9d27-8b63787981bb"
   },
   "outputs": [],
   "source": [
    "emotion.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion.save_model('./models/test.json')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "84VnyiCkxM2O",
    "outputId": "b2a4c044-1920-431b-ed3f-5560e2055bec"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use('seaborn')\n",
    "figure = plt.figure(figsize=(15, 4))\n",
    "plt.plot(emotion.history_training.history['accuracy'], 'darkorange', label = 'Accuracy')\n",
    "plt.title(\"Accuracywhile training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "ngkfKyXyfVPN",
    "outputId": "ce085a6a-9529-4260-befe-fa2e4cbbe905"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(15, 4))\n",
    "plt.plot(emotion.history_training.history['loss'], 'darkblue', label = 'Loss')\n",
    "plt.title(\"Loss while training\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmCFN7YIgYsG",
    "outputId": "97883276-5b81-490b-a002-b6b0fbdd75fd"
   },
   "outputs": [],
   "source": [
    "emotion.m.evaluate(emotion.X_test, emotion.Y_test, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L4SW7XTxfb2"
   },
   "outputs": [],
   "source": [
    "y_pred = emotion.m.predict(emotion.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-Bzg9rLy4TT"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6nixaqUy89r",
    "outputId": "2602c5dc-7d98-4010-e13c-ad133ab04042"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AUkw2x0ezAKS"
   },
   "outputs": [],
   "source": [
    "y_test = np.argmax(emotion.Y_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LUXLYXxzKQq",
    "outputId": "ede919dc-34f1-4dcd-a287-14ce084cd732"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKhpVX5TzRwM",
    "outputId": "fa50701c-3392-44b0-8862-5bf532d41428"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as acc\n",
    "print(acc(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MHH5qrIh0P-"
   },
   "outputs": [],
   "source": [
    "res = tf.math.confusion_matrix(y_pred,y_test).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "FbZ0pOpGh57L",
    "outputId": "ed23220f-f9b5-4535-d7d8-20b06acc758f"
   },
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(res,\n",
    "                     index = ['sadness', 'anger', 'love', 'surprise', 'fear', 'joy'], \n",
    "                     columns = ['sadness', 'anger', 'love', 'surprise', 'fear', 'joy'])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "_q6oYtSmiI7Z",
    "outputId": "f3731b71-5429-42f8-9326-0b6b8589e53b"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "figure = plt.figure(figsize=(7, 7))\n",
    "sns.heatmap(cm, annot=True, cmap=plt.cm.Blues)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjNZhv6clyGi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7eOTMJlma4y"
   },
   "source": [
    "### Second Step:\n",
    "Using the model that has been trained to sort tweets based on sentiment with a rating ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UahrijBm5lP"
   },
   "outputs": [],
   "source": [
    "SriLankaTweets = \"./data/SriLankaTweets.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0YOpknDsfpN"
   },
   "outputs": [],
   "source": [
    "SriLankaTweets = pd.read_csv(SriLankaTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "mf8b6U-5szu9",
    "outputId": "f8ab68b2-e4c2-4936-9ef1-303180c7250b"
   },
   "outputs": [],
   "source": [
    "SriLankaTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "ydkHw5Ydq7DO",
    "outputId": "bc2ca757-1ac9-40fe-c561-112874ef6e3f"
   },
   "outputs": [],
   "source": [
    "SriLankaTweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpKdM_GFrE8u",
    "outputId": "66c1acfe-0d27-4f97-cc40-d01f0ac750ed"
   },
   "outputs": [],
   "source": [
    "SriLankaTweets['language'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOvgh5BSGyMi"
   },
   "source": [
    "#### Dataset pretreatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUuiJLaXrRJX"
   },
   "outputs": [],
   "source": [
    "SriLankaTweets = SriLankaTweets.loc[SriLankaTweets['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sU5ZaQZ7t2V-",
    "outputId": "ffceaad2-2f96-4511-ef0d-14dc37466069"
   },
   "outputs": [],
   "source": [
    "len(SriLankaTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBLP7h6SxT92",
    "outputId": "79c76c40-096a-4a3e-a036-166b8cce8121"
   },
   "outputs": [],
   "source": [
    "SriLankaTweets['tweet'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSw_1ShCD99y"
   },
   "outputs": [],
   "source": [
    "def preprocessingText(sentences):\n",
    "  sentences = sentences.apply(lambda sequence:\n",
    "                                              [ltrs.lower() for ltrs in sequence if ltrs not in string.punctuation])\n",
    "  sentences = sentences.apply(lambda wrd: ''.join(wrd))\n",
    "  return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(SriLankaTweets['tweet'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeHNlHcKmpll"
   },
   "outputs": [],
   "source": [
    "SriLankaTweets['tweet'] = preprocessingText(SriLankaTweets['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "OXiVJ6Oho2BP",
    "outputId": "76402578-e11a-428f-f938-5c8b0805b45b"
   },
   "outputs": [],
   "source": [
    "SriLankaTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPEEE_4F4pxk"
   },
   "outputs": [],
   "source": [
    "features = SriLankaTweets['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sbRP6CYu44Vj",
    "outputId": "2d4e66fc-f744-4823-cfcd-fccedd930d45"
   },
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2h6l1gA5t5U8",
    "outputId": "ce110904-d226-41e6-9439-e537fc16ca80"
   },
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPsBZvm3t7-l",
    "outputId": "2b461848-0ef9-40d8-aef3-ecf8212d30b3"
   },
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_w9kPXcxnnm_",
    "outputId": "5c995846-c10f-46ee-e26b-4f2ce4747535"
   },
   "outputs": [],
   "source": [
    "tweets = emotion.tokenizer.texts_to_sequences(features)\n",
    "tweets = np.array(tweets).reshape(-1)\n",
    "tweets = pad_sequences(tweets, maxlen= 63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1UsVu3VoNgQ",
    "outputId": "7805bca8-6a2e-4eca-f4cd-5aa16bcab1ce"
   },
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nUNpv4AByA5F",
    "outputId": "1410e864-1ece-424a-9db0-791e39945dd9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUZFmlutHNBi"
   },
   "source": [
    "##Using the sentiment analysis model:\n",
    "Using the trained sentiment analysis model, in order to analyze the sentiments of tweeters within the Sri Lanka dataset.\n",
    "Sentiment type and sentiment affiliation will be preserved for each Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svLR7m52rOIP"
   },
   "outputs": [],
   "source": [
    "sentiment_labels = ['sadness', 'anger', 'love', 'surprise', 'fear', 'joy']\n",
    "sentiment_labels_encoding = [0, 1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qtloh5qbo61p"
   },
   "outputs": [],
   "source": [
    "results_sen_tweets = emotion.m.predict(tweets, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_sen_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJS57Je6r_4K"
   },
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "sentiment_labels1=[]\n",
    "sentiment_labels2=[]\n",
    "\n",
    "for i in results_sen_tweets:\n",
    "  res = np.argmax(i, axis = 0)\n",
    "  sentiments.append([sentiment_labels_encoding[res], i[res]])\n",
    "  sentiment_labels1.append(sentiment_labels_encoding[res])\n",
    "  sentiment_labels2.append(sentiment_labels[res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= { \"tweets\":SriLankaTweets['tweet'],\n",
    "       \"labels_num\": sentiment_labels1, \n",
    "       \"labels_text\": sentiment_labels2 \n",
    "\n",
    "      }\n",
    "\n",
    "tweet_labels_df = pd.DataFrame(data)\n",
    "tweet_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkDZqj66q6uW",
    "outputId": "02eb7244-66a0-43be-a93f-140361d69061"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OiPtGRUHgtq"
   },
   "source": [
    "### A graph showing the distribution of tweeters' feelings regarding events in Sri Lanka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmpEAXENzSXA"
   },
   "outputs": [],
   "source": [
    "def count_sent(sentiments, depending_on):\n",
    "  c = 0\n",
    "  for i in sentiments:\n",
    "    if i[0] == depending_on:\n",
    "      c = c + 1\n",
    "  return c    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcyTuOOAzZAR"
   },
   "outputs": [],
   "source": [
    "arr = []\n",
    "for i in sentiment_labels_encoding:\n",
    "  arr.append(count_sent(sentiments, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lx37Doy13Orq",
    "outputId": "68b103fc-f3de-493c-e71c-f860576b88ae"
   },
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "QOfMqOIF3ceX",
    "outputId": "5c7fb67c-9f4e-44dc-82f8-2db9ec6c40b4"
   },
   "outputs": [],
   "source": [
    "plt.pie(arr, labels = sentiment_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYR431cyHyT0"
   },
   "source": [
    "####  The hierarchical distribution of each feeling:\n",
    "This stage aims to determine the degree of convergence in terms of the single feeling of the tweeters, depending on the aggregation process based on the Euclidean distance, which depends on the percentage of feeling classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_labels_df['labels_num'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "from PIL import Image\n",
    "import urllib\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out the words of a class i.e. ('sadness', 'anger', 'love', 'surprise', 'fear', 'joy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_filter ='sadness'\n",
    "\n",
    "filtered_words = ' '.join(text for text in tweet_labels_df['tweets'][tweet_labels_df['labels_text']==apply_filter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the image with the dataset\n",
    "# Mask = np.array(Image.open(requests.get('http://clipart-library.com/image_gallery2/Twitter-PNG-Image.png', stream=True).raw))\n",
    "Mask = np.array(Image.open('./images/Twitter-PNG-Image.png'))\n",
    "\n",
    "# We use the ImageColorGenerator library from Wordcloud \n",
    "# Here we take the color of the image and impose it over our wordcloud\n",
    "image_colors = ImageColorGenerator(Mask)\n",
    "\n",
    "# Now we use the WordCloud function from the wordcloud library \n",
    "wc = WordCloud(background_color='black', height=1500, width=4000,mask=Mask).generate(filtered_words)\n",
    "\n",
    "# Size of the image generated \n",
    "plt.figure(figsize=(10,20))\n",
    "\n",
    "# Here we recolor the words from the dataset to the image's color\n",
    "# recolor just recolors the default colors to the image's blue color\n",
    "# interpolation is used to smooth the image generated \n",
    "plt.imshow(wc.recolor(color_func=image_colors),interpolation=\"hamming\")\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Features from cleaned Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-Words Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words is a method to extract features from text documents. These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set. \n",
    "\n",
    "Consider a corpus (a collection of texts) called C of D documents {d1,d2…..dD} and N unique tokens extracted out of the corpus C. The N tokens (words) will form a list, and the size of the bag-of-words matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i).\n",
    "\n",
    "For example, if you have 2 documents-\n",
    "\n",
    "\n",
    "\n",
    "- D1: He is a lazy boy. She is also lazy.\n",
    "\n",
    "- D2: Smith is a lazy person.\n",
    "\n",
    "First, it creates a vocabulary using unique words from all the documents\n",
    "#### [‘He’ , ’She’ , ’lazy’ , 'boy’ ,  'Smith’  , ’person’] \n",
    "\n",
    "- Here, D=2, N=6\n",
    "\n",
    "\n",
    "\n",
    "- The matrix M of size 2 X 6 will be represented as:\n",
    "\n",
    "![bow](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/07/table.png)\n",
    "\n",
    "The above table depicts the training features containing term frequencies of each word in each document. This is called bag-of-words approach since the number of occurrence and not sequence or order of words matters in this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. \n",
    "\n",
    "Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "- TF: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
    "#### TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n",
    "- IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "#### IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Classifier as an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "text=  I enjoyed my journey on this flight.\n",
      "1/1 [==============================] - 0s 333ms/step\n",
      "{'Label': 'love', 'model probability score': array([[0.1411524 , 0.13343234, 0.23124272, 0.18643332, 0.15681672,\n",
      "        0.1509225 ]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from flask import jsonify, request\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "# import requests\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('./models/test.json', 'r')\n",
    "loaded_model = model_from_json(json_file.read())\n",
    "json_file.close()\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"./models/test.json.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "sentiment_labels = ['sadness', 'anger', 'love', 'surprise', 'fear', 'joy']\n",
    "sentiment_labels_encoding = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "\n",
    "def preprocessingText1(sentences):\n",
    "    sentences = sentences.apply(lambda sequence:\n",
    "                                              [ltrs.lower() for ltrs in sequence if ltrs not in string.punctuation])\n",
    "    sentences = sentences.apply(lambda wrd: ''.join(wrd))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "@app.route(\"/predict_sentiment\", methods=['POST'])\n",
    "def predict_sentiment():\n",
    "    try:\n",
    "        params = json.loads(request.get_data())\n",
    "        text = params.get(\"query\",text)\n",
    "    except Exception as e:\n",
    "        print('text= ', text)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tw = preprocessingText1(pd.Series(text))\n",
    "    tw = tokenizer.texts_to_sequences(tw)\n",
    "    tw = pad_sequences(tw, maxlen= 63)\n",
    "    prob =loaded_model.predict(tw)\n",
    "    idx=pd.Series(prob[0]).idxmax()\n",
    "    prob_score = prob[0]\n",
    "    print(prob_score)\n",
    "    return {'Label': sentiment_labels[idx], 'model probability score':prob}\n",
    "#     return jsonify({'Label': sentiment_labels[idx], \n",
    "#                     'model probability score':prob})\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(host='0.0.0.0', port=105)\n",
    "\n",
    "test_sentence1 = \"I enjoyed my journey on this flight.\"\n",
    "print(predict_sentiment(test_sentence1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Analysis and sorting of tweeters' feelings.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
